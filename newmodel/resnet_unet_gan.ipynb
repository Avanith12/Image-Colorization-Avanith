{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.color import rgb2lab, lab2rgb\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from fastai.vision.learner import create_body\n",
    "from fastai.vision.models.unet import DynamicUnet\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found test images: 101\n"
     ]
    }
   ],
   "source": [
    "test_paths = sorted(glob.glob(\"/home/huuthanhvy.nguyen001/Image-Colorization/Testllama/model_gan_unet/test_100images/*.jpg\"))\n",
    "\n",
    "print(\"Found test images:\", len(test_paths))  # Should print 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_res_unet(n_input=1, n_output=2, size=256):\n",
    "\n",
    "    model = resnet18(pretrained=True)\n",
    "\n",
    "    body = create_body(model, pretrained=True, n_in=n_input, cut=-2)\n",
    "\n",
    "    net_G = DynamicUnet(body, n_output, (size, size)).to(device)\n",
    "    \n",
    "    return net_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(net, init='norm', gain=0.02):\n",
    "    \n",
    "    def init_func(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and 'Conv' in classname:\n",
    "            if init == 'norm':\n",
    "                nn.init.normal_(m.weight.data, mean=0.0, std=gain)\n",
    "            elif init == 'xavier':\n",
    "                nn.init.xavier_normal_(m.weight.data, gain=gain)\n",
    "            elif init == 'kaiming':\n",
    "                nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "            \n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                nn.init.constant_(m.bias.data, 0.0)\n",
    "        elif 'BatchNorm2d' in classname:\n",
    "            nn.init.normal_(m.weight.data, 1., gain)\n",
    "            nn.init.constant_(m.bias.data, 0.)\n",
    "            \n",
    "    net.apply(init_func)\n",
    "    print(f\"model initialized with {init} initialization\")\n",
    "    return net\n",
    "\n",
    "def init_model(model, device):\n",
    "    model = model.to(device)\n",
    "    model = init_weights(model)\n",
    "    return model\n",
    "\n",
    "class GANLoss(nn.Module):\n",
    "    def __init__(self, gan_mode='vanilla', real_label=1.0, fake_label=0.0):\n",
    "        super().__init__()\n",
    "        self.register_buffer('real_label', torch.tensor(real_label))\n",
    "        self.register_buffer('fake_label', torch.tensor(fake_label))\n",
    "        if gan_mode == 'vanilla':\n",
    "            self.loss = nn.BCEWithLogitsLoss()\n",
    "        elif gan_mode == 'lsgan':\n",
    "            self.loss = nn.MSELoss()\n",
    "    \n",
    "    def get_labels(self, preds, target_is_real):\n",
    "        if target_is_real:\n",
    "            labels = self.real_label\n",
    "        else:\n",
    "            labels = self.fake_label\n",
    "        return labels.expand_as(preds)\n",
    "    \n",
    "    def __call__(self, preds, target_is_real):\n",
    "        labels = self.get_labels(preds, target_is_real)\n",
    "        loss = self.loss(preds, labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, input_c, num_filters=64, n_down=3):\n",
    "        super().__init__()\n",
    "        model = [self.get_layers(input_c, num_filters, norm=False)]\n",
    "        model += [self.get_layers(num_filters * 2 ** i, num_filters * 2 ** (i + 1), s=1 if i == (n_down-1) else 2) \n",
    "                          for i in range(n_down)] # the 'if' statement is taking care of not using\n",
    "                                                  # stride of 2 for the last block in this loop\n",
    "        model += [self.get_layers(num_filters * 2 ** n_down, 1, s=1, norm=False, act=False)] # Make sure to not use normalization or\n",
    "                                                                                             # activation for the last layer of the model\n",
    "        self.model = nn.Sequential(*model)                                                   \n",
    "        \n",
    "    def get_layers(self, ni, nf, k=4, s=2, p=1, norm=True, act=True): # when needing to make some repeatitive blocks of layers,\n",
    "        layers = [nn.Conv2d(ni, nf, k, s, p, bias=not norm)]          # it's always helpful to make a separate method for that purpose\n",
    "        if norm: layers += [nn.BatchNorm2d(nf)]\n",
    "        if act: layers += [nn.LeakyReLU(0.2, True)]\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainModel(nn.Module):\n",
    "    def __init__(self, net_G=None, lr_G=2e-4, lr_D=2e-4, \n",
    "                 beta1=0.5, beta2=0.999, lambda_L1=100.):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.lambda_L1 = lambda_L1\n",
    "        \n",
    "        if net_G is None:\n",
    "            self.net_G = init_model(Unet(input_c=1, output_c=2, n_down=8, num_filters=64), self.device)\n",
    "        else:\n",
    "            self.net_G = net_G.to(self.device)\n",
    "\n",
    "        self.net_D = init_model(PatchDiscriminator(input_c=3, n_down=3, num_filters=64), self.device)\n",
    "        \n",
    "        self.GANcriterion = GANLoss(gan_mode='vanilla').to(self.device)\n",
    "        self.L1criterion = nn.L1Loss()\n",
    "        self.opt_G = optim.Adam(self.net_G.parameters(), lr=lr_G, betas=(beta1, beta2))\n",
    "        self.opt_D = optim.Adam(self.net_D.parameters(), lr=lr_D, betas=(beta1, beta2))\n",
    "    \n",
    "    def set_requires_grad(self, model, requires_grad=True):\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = requires_grad\n",
    "        \n",
    "    def setup_input(self, data):\n",
    "        self.L = data['L'].to(self.device)\n",
    "        self.ab = data['ab'].to(self.device)\n",
    "        \n",
    "    def forward(self):\n",
    "        self.fake_color = self.net_G(self.L)\n",
    "    \n",
    "    def backward_D(self):\n",
    "        \n",
    "        fake_image = torch.cat([self.L, self.fake_color], dim=1)\n",
    "        fake_preds = self.net_D(fake_image.detach())\n",
    "        self.loss_D_fake = self.GANcriterion(fake_preds, False)\n",
    "        real_image = torch.cat([self.L, self.ab], dim=1)\n",
    "        real_preds = self.net_D(real_image)\n",
    "        self.loss_D_real = self.GANcriterion(real_preds, True)\n",
    "        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n",
    "        self.loss_D.backward()\n",
    "    \n",
    "    def backward_G(self):\n",
    "        fake_image = torch.cat([self.L, self.fake_color], dim=1)\n",
    "        fake_preds = self.net_D(fake_image)\n",
    "        self.loss_G_GAN = self.GANcriterion(fake_preds, True)\n",
    "        self.loss_G_L1 = self.L1criterion(self.fake_color, self.ab) * self.lambda_L1\n",
    "        self.loss_G = self.loss_G_GAN + self.loss_G_L1\n",
    "        self.loss_G.backward()\n",
    "    \n",
    "    def optimize(self):\n",
    "        self.forward()\n",
    "        self.net_D.train()\n",
    "        self.set_requires_grad(self.net_D, True)\n",
    "        self.opt_D.zero_grad()\n",
    "        self.backward_D()\n",
    "        self.opt_D.step()\n",
    "        \n",
    "        self.net_G.train()\n",
    "        self.set_requires_grad(self.net_D, False)\n",
    "        self.opt_G.zero_grad()\n",
    "        self.backward_G()\n",
    "        self.opt_G.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from skimage.color import rgb2lab\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from skimage.color import rgb2lab\n",
    "from torchvision import transforms\n",
    "\n",
    "# Output directories\n",
    "train_dir = \"/hpcstor6/scratch01/h/huuthanhvy.nguyen001/imagenet_train\"\n",
    "val_dir = \"/hpcstor6/scratch01/h/huuthanhvy.nguyen001/imagenet_val\"\n",
    "\n",
    "train_count = len([f for f in os.listdir(train_dir) if f.endswith('.jpg')])\n",
    "val_count = len([f for f in os.listdir(val_dir) if f.endswith('.jpg')])\n",
    "\n",
    "print(f\"Train images: {train_count}\")\n",
    "print(f\"Validation images: {val_count}\")\n",
    "\n",
    "\n",
    "SIZE = 256\n",
    "\n",
    "class ColorizationAugmentor(Dataset):\n",
    "    def __init__(self, image_paths, split='train', num_augmentations=1):\n",
    "        self.paths = image_paths\n",
    "        self.split = split\n",
    "        self.num_augmentations = num_augmentations\n",
    "\n",
    "    def get_transforms(self, random=True):\n",
    "        if self.split == 'train':\n",
    "            transform_list = [\n",
    "                transforms.Resize((SIZE, SIZE), Image.BICUBIC),\n",
    "            ]\n",
    "            if random:\n",
    "                transform_list.append(transforms.RandomRotation(degrees=(0, 180)))\n",
    "            return transforms.Compose(transform_list)\n",
    "        elif self.split == 'val':\n",
    "            return transforms.Resize((SIZE, SIZE), Image.BICUBIC)\n",
    "\n",
    "    def load_original_image(self, path):\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        return img\n",
    "\n",
    "    def transform_and_convert_to_lab(self, img, transform):\n",
    "        img_transformed = transform(img)\n",
    "        img_np = np.array(img_transformed)\n",
    "        img_lab = rgb2lab(img_np).astype(\"float32\")\n",
    "        img_lab = transforms.ToTensor()(img_lab)\n",
    "        L = img_lab[[0], ...] / 50. - 1.\n",
    "        ab = img_lab[[1, 2], ...] / 110.\n",
    "        return L, ab\n",
    "\n",
    "    def get_item_with_original(self, idx):\n",
    "        img = self.load_original_image(self.paths[idx])\n",
    "        if self.num_augmentations == 1:\n",
    "            transform = self.get_transforms(random=True)\n",
    "            L, ab = self.transform_and_convert_to_lab(img, transform)\n",
    "            return {'original': img, 'L': L, 'ab': ab}\n",
    "        else:\n",
    "            augmented = []\n",
    "            for _ in range(self.num_augmentations):\n",
    "                transform = self.get_transforms(random=True)\n",
    "                L, ab = self.transform_and_convert_to_lab(img, transform)\n",
    "                augmented.append({'original': img, 'L': L, 'ab': ab})\n",
    "            return augmented\n",
    "\n",
    "    def dataset_length(self):\n",
    "        return len(self.paths) * self.num_augmentations\n",
    "\n",
    "SIZE = 256\n",
    "\n",
    "class ColorizationAugmentor(Dataset):\n",
    "    def __init__(self, paths, split='train', num_augmentations=1):\n",
    "        self.paths = paths\n",
    "        self.split = split\n",
    "        self.num_augmentations = num_augmentations\n",
    "\n",
    "    def get_transforms(self, random=True):\n",
    "        if self.split == 'train':\n",
    "            transform_list = [\n",
    "                transforms.Resize((SIZE, SIZE), Image.BICUBIC),\n",
    "            ]\n",
    "            if random:\n",
    "                transform_list.append(transforms.RandomRotation(degrees=(0, 180)))\n",
    "            return transforms.Compose(transform_list)\n",
    "        elif self.split == 'val':\n",
    "            return transforms.Resize((SIZE, SIZE), Image.BICUBIC)\n",
    "\n",
    "    def load_original_image(self, path):\n",
    "        return Image.open(path).convert(\"RGB\")\n",
    "\n",
    "    def transform_and_convert_to_lab(self, img, transform):\n",
    "        img_transformed = transform(img)\n",
    "        img_np = np.array(img_transformed)\n",
    "        img_lab = rgb2lab(img_np).astype(\"float32\")\n",
    "        img_lab = transforms.ToTensor()(img_lab)\n",
    "        L = img_lab[[0], ...] / 50. - 1.\n",
    "        ab = img_lab[[1, 2], ...] / 110.\n",
    "        return L, ab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths) * self.num_augmentations\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_idx = idx // self.num_augmentations\n",
    "        img = self.load_original_image(self.paths[image_idx])\n",
    "        transform = self.get_transforms(random=True)\n",
    "        L, ab = self.transform_and_convert_to_lab(img, transform)\n",
    "        return {'L': L, 'ab': ab}\n",
    "\n",
    "def make_dataloaders(paths, split='train', batch_size=16, num_augmentations=1, num_workers=4):\n",
    "    dataset = ColorizationAugmentor(paths=paths, split=split, num_augmentations=num_augmentations)\n",
    "    shuffle = (split == 'train')\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "train_paths = sorted(glob.glob(\"/hpcstor6/scratch01/h/huuthanhvy.nguyen001/imagenet_train/*.jpg\"))\n",
    "val_paths = sorted(glob.glob(\"/hpcstor6/scratch01/h/huuthanhvy.nguyen001/imagenet_val/*.jpg\"))\n",
    "\n",
    "train_dl = make_dataloaders(paths=train_paths, split='train', num_augmentations=2)\n",
    "val_dl = make_dataloaders(paths=val_paths, split='val', num_augmentations=1)\n",
    "\n",
    "data = next(iter(train_dl))\n",
    "print(\"L shape:\", data['L'].shape)\n",
    "print(\"ab shape:\", data['ab'].shape)\n",
    "\n",
    "print(\"Train loader batch size:\", len(train_dl))\n",
    "print(\"Val loader batch size:\", len(val_dl))\n",
    "\n",
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.count, self.avg, self.sum = [0.] * 3\n",
    "    \n",
    "    def update(self, val, count=1):\n",
    "        self.count += count\n",
    "        self.sum += count * val\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def create_loss_meters():\n",
    "    loss_D_fake = AverageMeter()\n",
    "    loss_D_real = AverageMeter()\n",
    "    loss_D = AverageMeter()\n",
    "    loss_G_GAN = AverageMeter()\n",
    "    loss_G_L1 = AverageMeter()\n",
    "    loss_G = AverageMeter()\n",
    "    \n",
    "    return {'loss_D_fake': loss_D_fake,\n",
    "            'loss_D_real': loss_D_real,\n",
    "            'loss_D': loss_D,\n",
    "            'loss_G_GAN': loss_G_GAN,\n",
    "            'loss_G_L1': loss_G_L1,\n",
    "            'loss_G': loss_G}\n",
    "\n",
    "def update_losses(model, loss_meter_dict, count):\n",
    "    for loss_name, loss_meter in loss_meter_dict.items():\n",
    "        loss = getattr(model, loss_name)\n",
    "        loss_meter.update(loss.item(), count=count)\n",
    "\n",
    "def lab_to_rgb(L, ab):\n",
    "    \"\"\"\n",
    "    Takes a batch of images\n",
    "    \"\"\"\n",
    "    \n",
    "    L = (L + 1.) * 50.\n",
    "    ab = ab * 110.\n",
    "    Lab = torch.cat([L, ab], dim=1).permute(0, 2, 3, 1).cpu().numpy()\n",
    "    rgb_imgs = []\n",
    "    for img in Lab:\n",
    "        img_rgb = lab2rgb(img)\n",
    "        rgb_imgs.append(img_rgb)\n",
    "    return np.stack(rgb_imgs, axis=0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from skimage.color import lab2rgb\n",
    "from tqdm import tqdm\n",
    "\n",
    "def visualize(model, data, save=True):\n",
    "    model.net_G.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.setup_input(data)\n",
    "        model.forward()\n",
    "\n",
    "    model.net_G.train()\n",
    "    fake_color = model.fake_color.detach()\n",
    "    real_color = model.ab\n",
    "\n",
    "    L = model.L\n",
    "\n",
    "    fake_imgs = lab_to_rgb(L, fake_color)\n",
    "    real_imgs = lab_to_rgb(L, real_color)\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 8))\n",
    "\n",
    "    for i in range(5):\n",
    "\n",
    "        ax = plt.subplot(3, 5, i + 1)\n",
    "        ax.imshow(L[i][0].cpu(), cmap='gray')\n",
    "        ax.axis(\"off\")\n",
    "        ax = plt.subplot(3, 5, i + 1 + 5)\n",
    "        ax.imshow(fake_imgs[i])\n",
    "        ax.axis(\"off\")\n",
    "        ax = plt.subplot(3, 5, i + 1 + 10)\n",
    "        ax.imshow(real_imgs[i])\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if save:\n",
    "        fig.savefig(f\"colorization_{time.time()}.png\")\n",
    "        \n",
    "def log_results(loss_meter_dict):\n",
    "    for loss_name, loss_meter in loss_meter_dict.items():\n",
    "        print(f\"{loss_name}: {loss_meter.avg:.5f}\")\n",
    "\n",
    "\n",
    "def train_model(model, train_dl, epochs, display_every=200):\n",
    "    for e in range(epochs):\n",
    "        loss_meter_dict = create_loss_meters()  # Track losses for this epoch\n",
    "        i = 0\n",
    "        for data in tqdm(train_dl, desc=f\"Epoch {e+1}/{epochs} [Training]\"):\n",
    "            model.setup_input(data)\n",
    "            model.optimize()\n",
    "            update_losses(model, loss_meter_dict, count=data['L'].size(0))\n",
    "\n",
    "            i += 1\n",
    "            if i % display_every == 0:\n",
    "                print(f\"\\nEpoch {e+1}/{epochs} - Iteration {i}/{len(train_dl)}\")\n",
    "                log_results(loss_meter_dict)\n",
    "\n",
    "                visualize(model, data, save=True)\n",
    "\n",
    "        print(f\"\\n‚úÖ End of Epoch {e+1} - Training Losses:\")\n",
    "        \n",
    "        log_results(loss_meter_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# net_G = build_res_unet(n_input=1, n_output=2, size=256)\n",
    "\n",
    "# net_G.load_state_dict(torch.load(\"res18-unet.pt\", map_location=device))\n",
    "\n",
    "# model = MainModel(net_G=net_G)\n",
    "\n",
    "# train_model(model, train_dl, epochs=50)\n",
    "\n",
    "# #TRAIN MODELs function here, next time I have to edit the function so it will save the best models with the lowest lost.\n",
    "\n",
    "# torch.save(model.state_dict(), \"final_model_weights.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Generate output on your own set of images\n",
    "\n",
    "'''\n",
    "img_path = '/home/huuthanhvy.nguyen001/Image-Colorization/Testllama/model_gan_unet/test'\n",
    "print(img_path)\n",
    "paths = glob.glob(img_path + \"/*\")\n",
    "idxs = np.arange(len(paths))\n",
    "\n",
    "test_dl = make_dataloaders(paths=paths, split='val')\n",
    "for data in tqdm(test_dl):\n",
    "  model.setup_input(data)\n",
    "  model.optimize()\n",
    "  visualize(model, data, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.color import lab2rgb\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ------------------------------\n",
    "# Load test images\n",
    "# ------------------------------\n",
    "\n",
    "test_paths = sorted(glob.glob(\"/home/huuthanhvy.nguyen001/Image-Colorization/Testllama/model_gan_unet/test_100images/*.jpg\"))\n",
    "\n",
    "print(\"Found test images:\", len(test_paths))  # Should print 5\n",
    "\n",
    "test_dl = make_dataloaders(paths=test_paths, split='val', batch_size=1, num_augmentations=1)\n",
    "\n",
    "# ------------------------------\n",
    "# Load final trained model\n",
    "# ------------------------------\n",
    "net_G = build_res_unet(n_input=1, n_output=2, size=256)\n",
    "model = MainModel(net_G=net_G)\n",
    "model.load_state_dict(torch.load(\"/home/huuthanhvy.nguyen001/Image-Colorization/Testllama/model_gan_unet/final_model_weights.pt\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ------------------------------\n",
    "# LAB to RGB conversion\n",
    "# ------------------------------\n",
    "def lab_to_rgb(L, ab):\n",
    "    L = (L + 1.) * 50.\n",
    "    ab = ab * 110.\n",
    "    Lab = torch.cat([L, ab], dim=1).permute(0, 2, 3, 1).cpu().numpy()\n",
    "    rgb_imgs = [lab2rgb(img) for img in Lab]\n",
    "    return np.stack(rgb_imgs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_all_test_images(model, test_dl, test_paths=None, chunk_size=10):\n",
    "    model.eval()\n",
    "    L_list, fake_list, real_list, names = [], [], [], []\n",
    "\n",
    "    for idx, data in enumerate(test_dl):\n",
    "        data = {k: v.to(device) for k, v in data.items()}\n",
    "        with torch.no_grad():\n",
    "            model.setup_input(data)\n",
    "            model.forward()\n",
    "\n",
    "        fake_color = model.fake_color.detach()\n",
    "        real_color = model.ab\n",
    "        L = model.L\n",
    "\n",
    "        fake_imgs = lab_to_rgb(L, fake_color)\n",
    "        real_imgs = lab_to_rgb(L, real_color)\n",
    "\n",
    "        L_list.append(L[0][0].cpu())\n",
    "        fake_list.append(fake_imgs[0])\n",
    "        real_list.append(real_imgs[0])\n",
    "        if test_paths:\n",
    "            names.append(os.path.basename(test_paths[idx]))\n",
    "        else:\n",
    "            names.append(f\"Image {idx+1}\")\n",
    "\n",
    "    # Split into chunks\n",
    "    total = len(fake_list)\n",
    "    for start in range(0, total, chunk_size):\n",
    "        end = min(start + chunk_size, total)\n",
    "        fig, axs = plt.subplots(end - start, 3, figsize=(12, (end - start) * 4))\n",
    "\n",
    "        if end - start == 1:\n",
    "            axs = [axs]  # wrap\n",
    "\n",
    "        for i in range(start, end):\n",
    "            row = i - start\n",
    "            axs[row][0].imshow(L_list[i], cmap='gray')\n",
    "            axs[row][0].set_title(f\"{names[i]}: Grayscale Input\")\n",
    "            axs[row][0].axis(\"off\")\n",
    "\n",
    "            axs[row][1].imshow(fake_list[i])\n",
    "            axs[row][1].set_title(\"Predicted Color\")\n",
    "            axs[row][1].axis(\"off\")\n",
    "\n",
    "            axs[row][2].imshow(real_list[i])\n",
    "            axs[row][2].set_title(\"Ground Truth\")\n",
    "            axs[row][2].axis(\"off\")\n",
    "\n",
    "        \n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# üîç Run it\n",
    "visualize_all_test_images(model, test_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbatch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
